# Notebooks

1- Training a simple bi-gram models using counts and neural nets: 

[Count vs Neural Net Based Bi-gram models](1_count-vs-nn-bi-gram.ipynb)

2- Training count based n-gram models : 

- [N-gram models](2_n-gram.ipynb)

- [N-gram models (Long version)](2_n-gram-long.ipynb)

3- Training neural net based (single weight vector) n-gram models:

[NN N-gram models](3_nn-n-gram.ipynb)

4- Multi Layer Perceptron based language model:

[MLP LM model](4_mlp-lm.ipynb)

5- Multi Layer Perceptron based language model to GPU:

[GPU MLP LM model](5_mlp-lm-gpu.ipynb)

6- Use micro batches to train the MLP:

[Batch Gradient Descent MLP LM](6_mlp-lm-batch.ipynb)

7- Move MLP code to pytorch modules:

[MLP LM (Pytorch Modules)](7_mlp-lm-torch-module.ipynb)

8- Single headed attention model (No dropout):

[Multi headed attention](8_attention-lm.ipynb)

9- Multi-headed attention model:

[Multi headed attention](9_multi-headed-attention-lm.ipynb)